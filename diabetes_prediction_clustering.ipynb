{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8hLF6iza73_R",
        "myiw7DrP785R",
        "wOMOGL3K-E4P",
        "JTpqdr7z-j0B",
        "3AT3kPQa-8u8",
        "PQn20qMt_D-8",
        "fj9lbX5g_LkI",
        "UIHR-EmG-eO4",
        "c83Tjdxx_dD2",
        "5fHyMrfG_h7K",
        "6zfu0F0z_n0j",
        "IWLkz8MhO4tf"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshuaOcharan/diabetes-prediction-clustering/blob/main/diabetes_prediction_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/datasets/mathchi/diabetes-data-set/data"
      ],
      "metadata": {
        "id": "zOopCS-G6iE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "8hLF6iza73_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score, roc_auc_score, precision_score, recall_score\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.stats import ttest_ind"
      ],
      "metadata": {
        "id": "HXo5GN2D6jl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('diabetes.csv')"
      ],
      "metadata": {
        "id": "tMg0NWBd7DDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data comes from the National Institute of Diabetes and Digestive and Kidney Diseases. There are 768 individuals in this dataset with 9 features. Each individuals is a women of at least 21 years old and of Pima Indian Heritage. The data looks at different diagnostic parameters and whether or not the individual has Diabetes.\n",
        "\n",
        "The 9 features in this dataset are\n",
        " - Pregnancies\n",
        "     - The number of times an individual has been pregnant\n",
        " - Glucose\n",
        "     - Plasma glucose concentration 2 hours after an oral glucose tolerance test\n",
        " - BloodPressure\n",
        "     - Diastolic blood pressure (mm Hg)\n",
        " - SkinThickness\n",
        "     - Triceps skin fold thickness (mm)\n",
        " - Insulin\n",
        "     - 2-hour serum insulin (mu U/ml)\n",
        " - BMI\n",
        "     - Body Mass Index (weight in kg / (height in m)^2)\n",
        " - DiabetesPedigreeFunction\n",
        "     - Function that calculates likelihood of Diabetes based on family history\n",
        " - Age\n",
        "     - Age of the individual (years)\n",
        " - Outcome\n",
        "     - Class variable (0 = non-diabetic, 1 = diabetic)\n",
        "\n",
        "Our goal is to use these parameters and build machine learning models that predict the likeliness of an individual having Diabetes. Specifically we will be testing a variety of both supervised and unsupervised models like decision trees, random forest, support vector machines, and K-means clustering."
      ],
      "metadata": {
        "id": "kf94njunueUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning & EDA"
      ],
      "metadata": {
        "id": "myiw7DrP785R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descriptions"
      ],
      "metadata": {
        "id": "wd07kRXJ7-6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "IUrjdGKx7nIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "5U74Irjy7qtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "_FzD3lu57jG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Glucose, bp, skin thickness, insulin, abd BMI have min of 0. Should not be possible. Basically missing values."
      ],
      "metadata": {
        "id": "jPfxonAW8ND2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "-ROnaijymjsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing Values"
      ],
      "metadata": {
        "id": "CPqwxSTK_Zqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()\n",
        "#no missing values"
      ],
      "metadata": {
        "id": "_aFdbl4o7Yxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df == 0).sum()"
      ],
      "metadata": {
        "id": "TR4OMyvi8ibA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replace 0s with Na\n",
        "replace_na_columns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "for c in replace_na_columns:\n",
        "    df[c] = df[c].replace(0, np.nan)\n",
        "    print(c,':', df[c].isna().sum())"
      ],
      "metadata": {
        "id": "Vnf-D7Ru-RFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Would have liked to have been able to just drop na values, but SkinThickness and Insulin have far too many missing values to just drop.\n",
        "\n",
        "3 options:\n",
        "\n",
        " - Drop entire SkinThickness and Insulin columns (unideal)\n",
        " - replace missing values with mean/median values (easy but creates bias)\n",
        " - predictive imputation like KNN or regression (more work, but probably best option)\n",
        "\n",
        "First just perform EDA with missing values."
      ],
      "metadata": {
        "id": "hjYftzYx_Tvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_percent = df.isna().mean()*100\n",
        "missing_percent"
      ],
      "metadata": {
        "id": "Q-Z26kn-Csoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_percent.sort_values(ascending=False).plot(kind=\"bar\", figsize=(10,5))\n",
        "plt.ylabel('Percent Missing')"
      ],
      "metadata": {
        "id": "1LGu6-yND2CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Variables & Visualization"
      ],
      "metadata": {
        "id": "wOMOGL3K-E4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pregnancies"
      ],
      "metadata": {
        "id": "JTpqdr7z-j0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Pregnancies'].hist()"
      ],
      "metadata": {
        "id": "ZKP9_dom-rMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Glucose"
      ],
      "metadata": {
        "id": "3AT3kPQa-8u8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Glucose'].hist()"
      ],
      "metadata": {
        "id": "GqXGuJfg-99p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R2tQcn8NvToz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Blood Pressure"
      ],
      "metadata": {
        "id": "PQn20qMt_D-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['BloodPressure'].hist()"
      ],
      "metadata": {
        "id": "gCtEJ0I__JMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Skin Thickness"
      ],
      "metadata": {
        "id": "fj9lbX5g_LkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['SkinThickness'].hist()"
      ],
      "metadata": {
        "id": "kY09E-45_NdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Insulin"
      ],
      "metadata": {
        "id": "UIHR-EmG-eO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Insulin'].hist()"
      ],
      "metadata": {
        "id": "Q57lhGAz-L7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DPF"
      ],
      "metadata": {
        "id": "c83Tjdxx_dD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['DiabetesPedigreeFunction'].hist()"
      ],
      "metadata": {
        "id": "lLo3JjNd_e1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Age"
      ],
      "metadata": {
        "id": "5fHyMrfG_h7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Age'].hist()"
      ],
      "metadata": {
        "id": "mRdGE-4B_k8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Outcome"
      ],
      "metadata": {
        "id": "6zfu0F0z_n0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Outcome'].value_counts().plot(kind = 'bar')"
      ],
      "metadata": {
        "id": "w902IvJB_q5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation Matrix"
      ],
      "metadata": {
        "id": "xpsihRouAAAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()"
      ],
      "metadata": {
        "id": "dD_CLZPTABnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outcome = 'Outcome'\n",
        "results = []\n",
        "for col in df.columns:\n",
        "    if col != outcome:\n",
        "        valid = df[[outcome, col]].dropna()\n",
        "        r, p = pearsonr(valid[outcome], valid[col])\n",
        "        results.append({\"Variable\": col,\n",
        "            \"Correlation\": round(r, 3),\n",
        "            \"p-value\": round(p, 4),\n",
        "            \"Significant (p<0.05)\": p < 0.05\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df"
      ],
      "metadata": {
        "id": "xL97wWIquGX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We find that all variables are significantly correlated with Outcome."
      ],
      "metadata": {
        "id": "neRagndJwg8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df, hue = 'Outcome', diag_kind = 'hist')"
      ],
      "metadata": {
        "id": "oTG6LhdsAYue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing Values cont."
      ],
      "metadata": {
        "id": "qR59H9okM3ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possible reason for high amount of missing values for insulin and skin thickness is because these factors aren't normally measured for people who aren't at risk of Diabetes. Glucose, BMI, and BP are more common factors that can help diagnose issues outside of Diabetes and therefore will likely be measured in most cases. We can check to see if whether a value is missing or not has effect on outcome."
      ],
      "metadata": {
        "id": "0MujD-RxNa5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Insulin_missing\"] = df[\"Insulin\"].isna().astype(int)\n",
        "sns.countplot(x=\"Outcome\", hue=\"Insulin_missing\", data=df)"
      ],
      "metadata": {
        "id": "VRxmeRcZNBT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for correlation\n",
        "insulin_cont = pd.crosstab(df['Outcome'], df['Insulin_missing'])\n",
        "chi2, p, dof, ex = stats.chi2_contingency(insulin_cont)\n",
        "print(f'p-value: {p:.4f}')"
      ],
      "metadata": {
        "id": "5r9m7x5NOjQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "p is 0.29 which is > 0.05 meaning Insulin_missingness is not correlated with Outcome."
      ],
      "metadata": {
        "id": "18qEnNNfO759"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['SkinThickness_missing'] = df['SkinThickness'].isna().astype(int)\n",
        "sns.countplot(x='Outcome', hue='SkinThickness_missing', data=df)"
      ],
      "metadata": {
        "id": "dEviE45SNMFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skin_cont = pd.crosstab(df['Outcome'], df['SkinThickness_missing'])\n",
        "chi2, p, dof, ex = stats.chi2_contingency(skin_cont)\n",
        "print(f'p-value: {p:.4f}')"
      ],
      "metadata": {
        "id": "1K8s40pPPJI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "p is 0.17 which is > 0.05 meaning SkinThickness_missing is not correlated with Outcome."
      ],
      "metadata": {
        "id": "oYAaXsF2PTxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few options:\n",
        "\n",
        " - Drop Insulin\n",
        " - Impute median\n",
        " - Regression\n",
        "\n",
        "Try two models with and without missing to compare and then decide."
      ],
      "metadata": {
        "id": "B3YiLZMMAuaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns = ['Insulin_missing', 'SkinThickness_missing'])"
      ],
      "metadata": {
        "id": "xcrFdChjQfPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "qQOMtE2_BF3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns = ['Outcome'])\n",
        "y = df['Outcome']"
      ],
      "metadata": {
        "id": "n0xoQ8fMBFj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify = y)\n",
        "results = {}\n",
        "\n",
        "# Model 1: Drop Insulin\n",
        "X_train_ModelA = X_train.drop(columns = ['Insulin'])\n",
        "X_test_ModelA = X_test.drop(columns = ['Insulin'])\n",
        "dtree_ModelA = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "dtree_ModelA.fit(X_train_ModelA, y_train)\n",
        "y_pred_ModelA = dtree_ModelA.predict(X_test_ModelA)\n",
        "results[\"Drop Insulin\"] = (accuracy_score(y_test, y_pred_ModelA), f1_score(y_test, y_pred_ModelA))\n",
        "\n",
        "\n",
        "# Model 2: Impute Median\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_train_ModelB = imputer.fit_transform(X_train)\n",
        "X_test_ModelB = imputer.transform(X_test)\n",
        "dtree_ModelB = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "dtree_ModelB.fit(X_train_ModelB, y_train)\n",
        "y_pred_ModelB = dtree_ModelB.predict(X_test_ModelB)\n",
        "results[\"Impute Median\"] = (accuracy_score(y_test, y_pred_ModelB), f1_score(y_test, y_pred_ModelB))\n",
        "\n",
        "for k, (acc, f1) in results.items():\n",
        "    print(f\"{k}: Accuracy = {acc:.3f}, F1 = {f1:.3f}\")"
      ],
      "metadata": {
        "id": "e3RtAWPGv4KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy and F1 are same across both models. Insulin likely has no effect on the model. We can check feature importance."
      ],
      "metadata": {
        "id": "L8gmNkhtMOUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#How much is Insulin affecting Decision Tree\n",
        "fi = pd.Series(dtree_ModelB.feature_importances_, index=X_train.columns)\n",
        "print(fi.sort_values(ascending=False))\n",
        "\n",
        "zero_features = fi[fi == 0].index.tolist()\n",
        "print(\"\\nZero-importance features:\", zero_features)"
      ],
      "metadata": {
        "id": "bYXysKi7LZcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Try model with 0 importance features dropped\n",
        "\n",
        "X_train_reduced = X_train.drop(columns = zero_features)\n",
        "X_test_reduced = X_test.drop(columns = zero_features)\n",
        "\n",
        "dtree_reduced = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "dtree_reduced.fit(X_train_reduced, y_train)\n",
        "y_pred_reduced = dtree_reduced.predict(X_test_reduced)\n",
        "acc = accuracy_score(y_test, y_pred_reduced)\n",
        "f1  = f1_score(y_test, y_pred_reduced)\n",
        "print(f'Accuracy: {acc:.3f}\\nF1: {f1:.3f}')"
      ],
      "metadata": {
        "id": "ra-vM5zV5dAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy and F1 did not change after dropping all zero importance features.\n",
        "\n",
        " Next test other depths to see which model is best."
      ],
      "metadata": {
        "id": "YzABjzb96DBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing tree depth\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "results = []\n",
        "\n",
        "for d in range(1,7):\n",
        "    dtree = DecisionTreeClassifier(max_depth = d, random_state = 0)\n",
        "    scores = cross_val_score(dtree, X_train_reduced, y_train, cv = cv, scoring = 'f1')\n",
        "    results.append((d, scores.mean()))\n",
        "\n",
        "for d, f1 in results:\n",
        "    print(f'Depth {d}: F1 = {f1:.3f}')\n",
        "\n",
        "best_depth, best_f1 = max(results, key=lambda x: x[1])\n",
        "print(f'\\nBest depth: {best_depth}\\nBest F1: {best_f1:.3f} ')"
      ],
      "metadata": {
        "id": "1y07Jt2r6o4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depth of 4 returns highest F1 value."
      ],
      "metadata": {
        "id": "KlfA9qHC8Qhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dtree with max_depth = 4\n",
        "\n",
        "dtree_reduced = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
        "dtree_reduced.fit(X_train_reduced, y_train)\n",
        "y_pred_reduced = dtree_reduced.predict(X_test_reduced)\n",
        "acc = accuracy_score(y_test, y_pred_reduced)\n",
        "f1  = f1_score(y_test, y_pred_reduced)\n",
        "print(f'Accuracy: {acc:.3f}\\nF1: {f1:.3f}')"
      ],
      "metadata": {
        "id": "K0YpIiGX8U8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_reduced)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
        "disp.plot(cmap='Blues')"
      ],
      "metadata": {
        "id": "kkENJo0g98BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare with baseline naive model."
      ],
      "metadata": {
        "id": "R84nuKSoBNs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Majority Class Baseline Model\n",
        "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
        "dummy.fit(X_train_reduced, y_train)\n",
        "y_dummy = dummy.predict(X_test_reduced)\n",
        "\n",
        "\n",
        "print(f'Accuracy: {round(accuracy_score(y_test, y_dummy), 2)}')\n",
        "print(f'F1: {f1_score(y_test, y_dummy)}')"
      ],
      "metadata": {
        "id": "8qWQgLTjBPEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision tree outperforms baseline majority class model."
      ],
      "metadata": {
        "id": "qZrUMo8jgfbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "aaiXBLagNvIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline Random Forest Model\n",
        "\n",
        "# Initialize and train the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(random_state=0, n_estimators=300)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# F1-score\n",
        "print(f\"F1-score : {f1_score(y_test, y_pred):.3f}\")\n"
      ],
      "metadata": {
        "id": "k2pB1hsh2F8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuning hyperparameters to find optimal settings\n",
        "parameter_grid = {\n",
        "    \"n_estimators\": [100, 300, 500],\n",
        "    \"max_depth\": [None, 10, 20],\n",
        "    \"max_features\": [\"sqrt\", \"log2\"],\n",
        "    \"class_weight\": [None, \"balanced\"]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "\n",
        "grid_search = GridSearchCV(rf, parameter_grid, cv=5, scoring=\"f1\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Setting model w/ ideal params and predicting\n",
        "best_model = grid_search.best_estimator_\n",
        "best_f1 = grid_search.best_score_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Results\n",
        "print(\"Best params:\", grid_search.best_params_)\n",
        "print(\"CV F1:\", round(grid_search.best_score_, 3))\n",
        "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred),3))\n",
        "print(\"Precision:\", round(precision_score(y_test, y_pred),3))\n",
        "print(\"Recall:\", round(recall_score(y_test, y_pred),3))\n",
        "print(\"F1:\", round(best_f1,3))"
      ],
      "metadata": {
        "id": "ZwT5rBdQ6rF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using GridSearchCV to find best parameters. Using direct predcitions (YES/NO from 50% probability threshold). Next: change threshold to see if we can get increased performance."
      ],
      "metadata": {
        "id": "oWZ-6nllGApn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Probabilities for the positive class (diabetes = 1)\n",
        "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Get precision, recall for many thresholds\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Graph Precision/Recall Tradeoff Graph\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(thresholds, precisions[:-1], color=\"blue\", label=\"Precision\")\n",
        "plt.plot(thresholds, recalls[:-1], color=\"green\", label=\"Recall\")\n",
        "plt.xlabel(\"Decision Threshold\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Precision-Recall Tradeoff\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Finding threshold with max F1 score\n",
        "thresholds = thresholds\n",
        "f1_scores = []\n",
        "\n",
        "for t in thresholds:\n",
        "    y_pred_custom = (y_prob >= t).astype(int)\n",
        "    f1_scores.append(f1_score(y_test, y_pred_custom))\n",
        "\n",
        "best_t = thresholds[f1_scores.index(max(f1_scores))]\n",
        "print(\"Best threshold for F1:\", best_t)"
      ],
      "metadata": {
        "id": "gO9bE7bYMdUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_custom = (y_prob >= best_t).astype(int) # Taking y_probs from best model from GridCVSearch and comparing to chosen threshold\n",
        "\n",
        "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_custom),3))\n",
        "print(\"Precision:\", round(precision_score(y_test, y_pred_custom),3))\n",
        "print(\"Recall:\", round(recall_score(y_test, y_pred_custom),3))\n",
        "print(\"F1:\", round(f1_score(y_test, y_pred_custom),3))"
      ],
      "metadata": {
        "id": "Ekxig6ztN-2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improved Recall compared to F1 compared to previous model. This is important for our application (Diabetes prediction) since recall is extremely important in healthcare diagnostics."
      ],
      "metadata": {
        "id": "i9JI3dDEPDIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare RF Model to Dummy\n",
        "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
        "dummy.fit(X_train, y_train)\n",
        "\n",
        "y_dummy = dummy.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Baseline Accuracy:\", round(accuracy_score(y_test, y_dummy),3))\n",
        "print(\"Baseline F1:\", f1_score(y_test, y_dummy))"
      ],
      "metadata": {
        "id": "A-o9oXBsJ8gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirmation that RF model outperforms dummy model."
      ],
      "metadata": {
        "id": "IIij0VFgPfzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrices\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "cm_dt = confusion_matrix(y_test, y_pred_reduced)\n",
        "ConfusionMatrixDisplay(cm_dt, display_labels=[0, 1]).plot(ax=axes[0], cmap='Blues')\n",
        "axes[0].set_title(\"Decision Tree\")\n",
        "\n",
        "cm_rf = confusion_matrix(y_test, y_pred)\n",
        "ConfusionMatrixDisplay(cm_rf, display_labels=[0,1]).plot(ax=axes[1], cmap='Blues')\n",
        "axes[1].set_title(\"Random Forest\")"
      ],
      "metadata": {
        "id": "RS9Xt_FreP5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrices show that random forest model has much better precision and recall."
      ],
      "metadata": {
        "id": "BXamKxXPS-78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feat_importance = pd.Series(best_model.feature_importances_, index=X_train.columns)\n",
        "feat_importance.sort_values().plot(kind=\"barh\", figsize=(8,5))\n",
        "plt.title(\"Random Forest Feature Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Em0PDtxTi5a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All features contributing to random forest model unlike decision tree."
      ],
      "metadata": {
        "id": "0lwdf3vsTEH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machines"
      ],
      "metadata": {
        "id": "1EIV5Rz5ita1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# cols w/ 0s\n",
        "cols_with_zeros_as_nan = [\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]\n",
        "\n",
        "\n",
        "# replace 0s w/ NaN\n",
        "df_clean = df.copy()\n",
        "df_clean[cols_with_zeros_as_nan] = df_clean[cols_with_zeros_as_nan].replace(0, np.nan)\n",
        "\n",
        "#fill missing values w/ median\n",
        "X = df_clean.drop(columns=\"Outcome\").fillna(df_clean.median())\n",
        "y = df_clean[\"Outcome\"]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# pipeline: standardize features before fitting\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm\", SVC())\n",
        "])"
      ],
      "metadata": {
        "id": "qm6ywq8UjPCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# param combinations for tests\n",
        "param_grid = {\n",
        "    \"svm__C\": [0.1, 1, 10],\n",
        "    \"svm__kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
        "    \"svm__gamma\": [\"scale\", \"auto\"]\n",
        "}\n",
        "\n",
        "# cross-validation for each combination\n",
        "grid = GridSearchCV(pipe, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Best CV accuracy:\", grid.best_score_)"
      ],
      "metadata": {
        "id": "CPDSMooz5OmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "ConfusionMatrixDisplay.from_estimator(grid, X_test, y_test, cmap=\"Blues\")\n",
        "plt.title(\"SVM Confusion Matrix (Test Set)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uPKzkXyb8TMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# prepare data\n",
        "X = df_clean.drop(columns=\"Outcome\").fillna(df_clean.median())\n",
        "y = df_clean[\"Outcome\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# models\n",
        "models = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"SVM (RBF, scaled)\": Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm\", SVC(kernel=\"rbf\", C=1, gamma=\"scale\"))\n",
        "    ])\n",
        "}\n",
        "\n",
        "# train, predict, get scores\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1\": f1_score(y_test, y_pred)\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results).set_index(\"Model\")\n",
        "display(results_df)"
      ],
      "metadata": {
        "id": "QX2vZxRYPTqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table above compares the performance of the three models covered so far: Decision Tree, Random Forest, and SVM."
      ],
      "metadata": {
        "id": "ffCiQgoqjycj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_decision_boundary(model, X, y, features):\n",
        "    X_2d = X[features].values\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_2d)\n",
        "\n",
        "    # fit model\n",
        "    model.fit(X_scaled, y)\n",
        "    # create grid of points covering feature space\n",
        "    x_min, x_max = X_scaled[:, 0].min()-1, X_scaled[:, 0].max()+1\n",
        "    y_min, y_max = X_scaled[:, 1].min()-1, X_scaled[:, 1].max()+1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                         np.linspace(y_min, y_max, 200))\n",
        "\n",
        "    # predict class for each point\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "    # plot\n",
        "    plt.contourf(xx, yy, Z, alpha=0.2)\n",
        "    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, edgecolors=\"k\")\n",
        "    plt.xlabel(features[0]); plt.ylabel(features[1])\n",
        "    plt.title(\"SVM Decision Boundary\")\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(SVC(kernel=\"rbf\", C=grid.best_params_[\"svm__C\"]), X_train, y_train, [\"Glucose\",\"BMI\"])"
      ],
      "metadata": {
        "id": "00eYbWUE8Vax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2D visualization of how SVM separates classes in Glucose-BMI space.\n",
        "\n",
        "* Shaded regions show SVM decision boundary\n",
        "\n",
        "* Scatter points show real patient data\n",
        "\n",
        "This helps interpret whether model is capturing meaningful separation (like higher glucose + higher BMI = more likely diabetic)."
      ],
      "metadata": {
        "id": "7QnLXoem8f77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means Clustering"
      ],
      "metadata": {
        "id": "IWLkz8MhO4tf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "uqx7u4z8VV88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "JR8rY1qfTka8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop Outcome column to get rid of labels in unsupervised learning. Handle NaNs"
      ],
      "metadata": {
        "id": "cmbXsYOBVYDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = [c for c in df.columns if c != 'Outcome']\n",
        "x = df[feature_cols].copy()\n",
        "y = df['Outcome'].copy()\n",
        "\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "scaler = StandardScaler()\n",
        "x_imputed = imputer.fit_transform(x)\n",
        "x_scaled = scaler.fit_transform(x_imputed)"
      ],
      "metadata": {
        "id": "nzZ9Gk-UUXtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try different k values"
      ],
      "metadata": {
        "id": "kZs87uSZVmCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ks = [2, 3, 4, 5, 6]\n",
        "inertias, silhouettes = [], []\n",
        "\n",
        "for k in ks:\n",
        "  km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
        "  labels = km.fit_predict(x_scaled)\n",
        "  inertias.append(km.inertia_)\n",
        "  silhouettes.append(silhouette_score(x_scaled, km.labels_))\n",
        "\n",
        "best_k = ks[int(np.argmax(silhouettes))]\n",
        "print(\"Silhouette Scores: \", dict(zip(ks, [round(s,4) for s in silhouettes])))\n",
        "print(\"Chosen k: \", best_k)"
      ],
      "metadata": {
        "id": "LX-wDqbzUoa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit final KMeans"
      ],
      "metadata": {
        "id": "C-vui46-VoGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=best_k, n_init=25, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(x_scaled)\n",
        "\n",
        "df_kmeans = df.copy()\n",
        "df_kmeans['cluster'] = cluster_labels"
      ],
      "metadata": {
        "id": "pm6NfubXVQw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "id": "oZ6vPYG0V6wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(ks, inertias, marker='o')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(ks, silhouettes, marker='o')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score')\n",
        "plt.show()\n",
        "\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "x_pca = pca.fit_transform(x_scaled)\n",
        "plt.figure(figsize=(6,5))\n",
        "for c in range(best_k):\n",
        "  mask = cluster_labels == c\n",
        "  plt.scatter(x_pca[mask, 0], x_pca[mask, 1], s=20, label=f'Cluster {c}')\n",
        "plt.scatter(\n",
        "    pca.transform(kmeans.cluster_centers_)[:, 0],\n",
        "    pca.transform(kmeans.cluster_centers_)[:, 1],\n",
        "    marker='x', s=150, c='black', label='Centroids'\n",
        ")\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('K-Means Clustering')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9GYNETXzV9Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate vs Outcome"
      ],
      "metadata": {
        "id": "3N2Gs6HwW-Uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ct = pd.crosstab(df_kmeans['cluster'], y, rownames=['Cluster'], colnames=['Outcome'])\n",
        "purity = (ct.max(axis=1).sum()) / ct.to_numpy().sum()\n",
        "ari = adjusted_rand_score(y, cluster_labels)\n",
        "\n",
        "print(\"Cluster vs Outcome table:\")\n",
        "print(ct)\n",
        "print(f\"Purity: {round(purity, 4)}\")\n",
        "print(f\"ARI: {round(ari, 4)}\")"
      ],
      "metadata": {
        "id": "VLeGou1QXES2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect centroids in original scale"
      ],
      "metadata": {
        "id": "slopCicoXrNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "centroids = scaler.inverse_transform(kmeans.cluster_centers_)\n",
        "centroids_df = pd.DataFrame(centroids, columns=x.columns)\n",
        "centroids_df['cluster'] = range(best_k)\n",
        "print(\"\\nCentroids in original scale:\")\n",
        "print(centroids_df)"
      ],
      "metadata": {
        "id": "EP6xkItRXtzp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}